# -*- coding: utf-8 -*-
"""DataCleaning of projetofinalvc.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tuxv1PHU-ORo9HmrgvUV3kKRQVi5lHAJ

# Projeto Final de Visualização Científica

# A. Dataset

Será feita uma análise de um subconjunto dos dados do dataset do [paper](https://dl.acm.org/citation.cfm?id=3345639) "From Reports to Bug-Fix Commits: A 10 Years Dataset of Bug-Fixing Activity from 55 Apache's Open Source Projects" Renan Vieira, Antônio da Silva, Lincoln Rocha, and João Paulo Gomes. 2019. O subconjunto escolhido foi referente aos dados do framework **Hadoop** (**Core**, **Mapreduce**, **HDFS** e **Yarn**). Serão analisados os dados de bug-fix para identificar padrões e comportamentos da resolução dos bugs nesse framework.

## 1. Links dos dados

Snapshot

[Github Hadoop Bug-fix](https://raw.githubusercontent.com/scientific-visualization/final/master/snapshot/hadoop-full-bug-fix-dataset-pv.csv)

[Github Mapreduce Bug-fix](https://raw.githubusercontent.com/scientific-visualization/final/master/snapshot/mapreduce-full-bug-fix-dataset-pv.csv)

[Github HDFS](https://raw.githubusercontent.com/scientific-visualization/final/master/snapshot/hdfs-full-bug-fix-dataset-pv.csv)

[Github YARN](https://raw.githubusercontent.com/scientific-visualization/final/master/snapshot/yarn-full-bug-fix-dataset-pv.csv)
"""

URL_SNAPSHOT_HADOOP = "https://raw.githubusercontent.com/scientific-visualization/final/master/snapshot/hadoop-full-bug-fix-dataset-pv.csv"
URL_SNAPSHOT_MAPREDUCE = "https://raw.githubusercontent.com/scientific-visualization/final/master/snapshot/mapreduce-full-bug-fix-dataset-pv.csv"
URL_SNAPSHOT_HDFS = "https://raw.githubusercontent.com/scientific-visualization/final/master/snapshot/hdfs-full-bug-fix-dataset-pv.csv"
URL_SNAPSHOT_YARN = "https://raw.githubusercontent.com/scientific-visualization/final/master/snapshot/yarn-full-bug-fix-dataset-pv.csv"

URL_CHANGELOG_HADOOP = "https://raw.githubusercontent.com/scientific-visualization/final/master/changelog/hadoop-bug-fix-changelog-dataset.csv"
URL_CHANGELOG_MAPREDUCE = "https://raw.githubusercontent.com/scientific-visualization/final/master/changelog/mapreduce-bug-fix-changelog-dataset.csv"
URL_CHANGELOG_HDFS = "https://raw.githubusercontent.com/scientific-visualization/final/master/changelog/hdfs-bug-fix-changelog-dataset.csv"
URL_CHANGELOG_YARN = "https://raw.githubusercontent.com/scientific-visualization/final/master/changelog/yarn-bug-fix-changelog-dataset.csv"

URL_COMMIT_LOG_HADOOP = "https://raw.githubusercontent.com/scientific-visualization/final/master/commit-log/hadoop-bug-fix-commit-log-dataset.csv"
URL_COMMIT_LOG_MAPREDUCE = "https://raw.githubusercontent.com/scientific-visualization/final/master/commit-log/mapreduce-bug-fix-commit-log-dataset.csv"
URL_COMMIT_LOG_HDFS = "https://raw.githubusercontent.com/scientific-visualization/final/master/commit-log/hdfs-bug-fix-commit-log-dataset.csv"
URL_COMMIT_LOG_YARN = "https://raw.githubusercontent.com/scientific-visualization/final/master/commit-log/yarn-bug-fix-commit-log-dataset.csv"

URL_COMMENT_LOG_HADOOP = "https://raw.githubusercontent.com/scientific-visualization/final/master/comment-log/hadoop-bug-fix-comment-log-dataset.csv"
URL_COMMENT_LOG_MAPREDUCE = "https://raw.githubusercontent.com/scientific-visualization/final/master/comment-log/mapreduce-bug-fix-comment-log-dataset.csv"
URL_COMMENT_LOG_HDFS = "https://raw.githubusercontent.com/scientific-visualization/final/master/comment-log/hdfs-bug-fix-comment-log-dataset.csv"
URL_COMMENT_LOG_YARN = "https://raw.githubusercontent.com/scientific-visualization/final/master/comment-log/yarn-bug-fix-comment-log-dataset.csv"

URL_TARGET_PROJECTS_LIBS = "https://raw.githubusercontent.com/scientific-visualization/final/master/target-projects-list.csv"

"""## 2. Loadind Dataset"""

import pandas as pd

df_snapshot_hadoop = pd.read_csv(URL_SNAPSHOT_HADOOP, sep=';')
df_snapshot_mapreduce = pd.read_csv(URL_SNAPSHOT_MAPREDUCE, sep=';')
df_snapshot_hdfs = pd.read_csv(URL_SNAPSHOT_HDFS, sep=';')
df_snapshot_yarn = pd.read_csv(URL_SNAPSHOT_YARN, sep=';')

df_changelog_hadoop = pd.read_csv(URL_CHANGELOG_HADOOP, ';')
df_changelog_mapreduce = pd.read_csv(URL_CHANGELOG_MAPREDUCE, sep=';')
df_changelog_hdfs = pd.read_csv(URL_CHANGELOG_HDFS, sep=';')
df_changelog_yard = pd.read_csv(URL_CHANGELOG_YARN, sep=';')

df_commitlog_hadoop = pd.read_csv(URL_COMMIT_LOG_HADOOP, sep=';')
df_commitlog_mapreduce = pd.read_csv(URL_COMMIT_LOG_MAPREDUCE, sep=';')
df_commitlog_hdfs = pd.read_csv(URL_COMMIT_LOG_HDFS, sep=';')
df_commitlog_yarn = pd.read_csv(URL_COMMIT_LOG_YARN, sep=';')

df_commentlog_hadoop = pd.read_csv(URL_COMMENT_LOG_HADOOP, sep=';')
df_commentlog_mapreduce = pd.read_csv(URL_COMMENT_LOG_MAPREDUCE, sep=';')
df_commentlog_hdfs = pd.read_csv(URL_COMMENT_LOG_HDFS, sep=';')
df_commentlog_yard = pd.read_csv(URL_COMMENT_LOG_YARN, sep=';')

df_target_projects_libs = pd.read_csv(URL_TARGET_PROJECTS_LIBS)

"""## 3. Routines

### Imports
"""

import seaborn as sns
from datetime import datetime
import matplotlib.pyplot as plt

from dateutil.parser import parse

import plotly.offline as py
import plotly.graph_objs as go

import numpy as np
import random
import plotly

py.init_notebook_mode(connected=False)

"""### Functions"""

# Show all rows from DataFrame
def print_full(myDF):
  try:
    with pd.option_context("display.max_rows", myDF.shape[0]):
      display(myDF)
  except Exception as e:
    print("type erro:" + str(e))

# Show rows with NaN content from DataFrame
def show_rows_with_NaN(myDF):
  try:
    df1 = myDF[myDF.isna().any(axis=1)]
  except Exception as e:
    print("type erro:" + str(e))
  return df1

# Specific convet to DateTime (01 <= Year <= 19)
def convert_to_DateTime(myDF, column):
  try: 
    for i in range(myDF.shape[0]):
      dateAux = myDF.loc[i, column]
      splitDateAux = dateAux.split("/") 
      yearAux = splitDateAux[2]
      year = int(yearAux) + 2000
      date = splitDateAux[0] + "/" + splitDateAux[1] + "/" + str(year)
      myDF.loc[i, column] = datetime.strptime(date, "%m/%d/%Y")
  except Exception as e:
    print("Error: " + str(e))
  return myDF

def create_dataset_sum_groupby_column(myDF, myColumn):
	group_by = myDF.groupby(myColumn)

	lista = []
	for name, group in group_by:
	  elemento = []
	  qtd = len(group)
	  elemento.append(name)
	  elemento.append(qtd)
	  lista.append(elemento)
  
	## Cria um dataset auxiliar para mostrar os itens que mais quantidade
	df_issue = pd.DataFrame(lista,columns=[myColumn,'Qtd'])

	newDFsummarized = df_issue.sort_values(by=['Qtd'], ascending=False)
	return newDFsummarized

def format_string_to_date(myDF, myColumn):
  try:
    for i in range(myDF.shape[0]): 
      date_time_obj = parse(myDF.loc[i, myColumn]) 
      myDF.loc[i, myColumn] = date_time_obj.date()
  except Exception as e:
    print("Error: " + str(e))
  return myDF

# Devido o Plotly estar sendo utilizado no Google Collab, precisaremos definir
# a função abaixo e chamá-la sempre que quisermos exibir um gráfico

def configure_plotly_browser_state():
  import IPython
  display(IPython.core.display.HTML('''
        <script src="/static/components/requirejs/require.js"></script>
        <script>
          requirejs.config({
            paths: {
              base: '/static/base',
              plotly: 'https://cdn.plot.ly/plotly-1.5.1.min.js?noext',
            },
          });
        </script>
        '''))

"""# B. Datasheet Analysis

## 1. Target_projects_libs

### 1.1 Dataframes
"""

df_target_projects_libs

df_target_projects_libs.isnull().sum()

"""**Tipos de dados do Dataframe df_target_projects_libs**"""

df_target_projects_libs.info()

"""**Dados estatísticos básicos do Dataframe df_target_projects_libs**"""

df_target_projects_libs.describe()

"""**Criação de variáveis para somatório dos bugs**"""

total_of_all_bugs = df_target_projects_libs['bugs'].sum()
bugs_hadoop = df_target_projects_libs.query('project in ["Hadoop Core", "Hadoop Yarn", "Hadoop HDFS", "Hadoop MapReduce"]')
total_of_all_bugs_hadoop = bugs_hadoop.bugs.sum()

"""**Derivação dos dados específicos do Framework Hadoop**"""

bugs_hadoop

"""### 1.2 Gráficos

**Percentual de bugs do Hadoop em relação a todos os bugs**
"""

configure_plotly_browser_state()

labels = ['All Bugs','Hadoop']
values = [total_of_all_bugs, total_of_all_bugs_hadoop]

fig = go.Figure(data=[go.Pie(labels=labels, values=values)])
py.iplot(fig)

"""**Relação de bugs entre os módulos do hadoop**"""

configure_plotly_browser_state()

labels = [bugs_hadoop.project[0], bugs_hadoop.project[1], bugs_hadoop.project[2], bugs_hadoop.project[3]]
values = [bugs_hadoop.bugs[0], bugs_hadoop.bugs[1], bugs_hadoop.bugs[2], bugs_hadoop.bugs[3]]

fig = go.Figure(data=[go.Pie(labels=labels, values=values)])
py.iplot(fig)

"""### 1.3 Observações gerais

Obs: análise dos dados do dataframe:


*   Foram identificados todos os tipos de dados
*   Não foram identificados dados faltantes
*   Não foram identificados dados nulos
*   Não foi necessário fazer transformações de dados
*   Foi criado um dataframe bugs_hadoop derivado do dataframe principal de todos os bugs para extrair informações específicas dos bugs do Hadoop.

## 2. Hadoop Bug-Fix

### 2.1 Snapshot

**1. It was removed the column Unnamed** *italicized text*
"""

df_snapshot_hadoop.drop(['Unnamed: 0'], axis=1, inplace=True)

"""**2. Fragment of Hadoop Snapshot Dataframe**"""

df_snapshot_hadoop.head(3)

"""**3. General information**"""

df_snapshot_hadoop.info()

"""**4. df_snapshot_hadoop Data is null**"""

df_snapshot_hadoop.isnull().sum()

"""**5. df_snapshot_hadoop Columns with data null**"""

columsn_with_null_elements = df_snapshot_hadoop.isna().any()[lambda x: x]
columsn_with_null_elements

null_cols_df_snapshot_hadoop = [item for item in df_snapshot_hadoop.columns if df_snapshot_hadoop[item].isnull().any()]
null_cols_df_snapshot_hadoop

df_snapshot_hadoop[null_cols_df_snapshot_hadoop].head(3)

"""**6. Replacing important columns null data values with unknown**"""

df_snapshot_hadoop.Assignee.fillna('unknown', inplace=True)
df_snapshot_hadoop.Components.fillna('unknown', inplace=True)
df_snapshot_hadoop.SummaryTopWords.fillna('unknown', inplace=True)
df_snapshot_hadoop.DescriptionTopWords.fillna('unknown', inplace=True)
df_snapshot_hadoop.CommentsTopWords.fillna('unknown', inplace=True)
df_snapshot_hadoop.AffectsVersions.fillna('unknown', inplace=True)
df_snapshot_hadoop.FixVersions.fillna('unknown', inplace=True)
df_snapshot_hadoop.CommitsMessagesTopWords.fillna('unknown', inplace=True)

df_snapshot_hadoop[null_cols_df_snapshot_hadoop]

df_snapshot_hadoop.isnull().sum()

column_with_lot_of_nulls = ['FirstAttachmentDate', 'LastAttachmentDate', 'FirstAttachedPatchDate', 'LastAttachedPatchDate', 'InwardIssueLinks', 'OutwardIssueLinks']

"""**7. Removing column_with_lot_of_nulls**"""

df_snapshot_hadoop.drop(column_with_lot_of_nulls, axis=1, inplace=True)

df_snapshot_hadoop.isnull().sum()

"""**8. Removing rows with null values and reseting main index**"""

df_snapshot_hadoop.dropna(how='any', inplace=True)
df_snapshot_hadoop.reset_index(drop=True, inplace=True)

df_snapshot_hadoop.isnull().sum()

"""**Procedures to create a WordCloud of SummaryTopWords**"""

## Cria uma lista com todas as palavras (palavra:quantidade) baseado na coluna SummaryTopWords
#indice de registros sem na/null na coluna SummaryTopWords
indice_notna = df_snapshot_hadoop.SummaryTopWords.notna()

lista = []
for elemento in df_snapshot_hadoop.SummaryTopWords[indice_notna]:
  listaAux = elemento.split()
  for elementoAux in listaAux:
    lista.append(elementoAux)
#lista com todos os elementos da coluna SummaryTopWords

# Refatorar para considerar palavras repetidas em um mesmo bug
# Cria uma lista apenas com as palavras da coluna SummaryTopWords
myList = []
for elemento in lista:
  listaAux = elemento.split(":")
  for elementoAux in listaAux:
    if (elementoAux not in ['1', '2', '3', '4', '5', '6', '7', '8', '10']):
      myList.append(elementoAux)
#myList

# Gera uma collection que conta a quantidade de vezes que a palavra apareceu
from collections import Counter
counts = Counter(myList)
#counts

# Gera duas listas separadas, uma lista contendo a palavra e outra lista contendo quantas vezes a palavra apareceu
lista_elemento = []
lista_valor = []
for elemento in counts:
  lista_elemento.append(elemento)
  lista_valor.append(counts[elemento]) 
  
my_lista_valor = []
for i in range(len(lista_valor)):
  my_lista_valor.append(len(lista_valor))  
#my_lista_valor

"""**WordCloud SummaryTopWords**"""

## worldclout in python
# https://www.datacamp.com/community/tutorials/wordcloud-python

## worldcloud in dash
# https://community.plot.ly/t/wordcloud-in-dash/11407
configure_plotly_browser_state()

words = lista_elemento
colors = [plotly.colors.DEFAULT_PLOTLY_COLORS[random.randrange(1, 10)] for i in range(30)]
weights = lista_valor

data = go.Scatter(x=[random.random() for i in range(100)],
                 y=[random.random() for i in range(100)],
                 mode='text',
                 text=words,
                 marker={'opacity': 0.3},
                 textfont={'size': weights,
                           'color': colors})
layout = go.Layout({'xaxis': {'showgrid': False, 'showticklabels': False, 'zeroline': False},
                    'yaxis': {'showgrid': False, 'showticklabels': False, 'zeroline': False}})
fig = go.Figure(data=[data], layout=layout)

py.iplot(fig)

"""**Cleaned data Aux to record df_snapshot_hadoop partial cleaned**"""

cleaned_aux_df_snapshot_hadoop = df_snapshot_hadoop

"""**9. Date transformation and creation of new columns about date**"""

df_snapshot_hadoop['CreationDate'] = pd.to_datetime(df_snapshot_hadoop['CreationDate'], dayfirst=True).dt.strftime('%Y-%m-%d %H:%M:%S')
df_snapshot_hadoop['CreationDate'] = pd.to_datetime(df_snapshot_hadoop['CreationDate'])

df_snapshot_hadoop['CreationDateDay'] = pd.to_datetime(df_snapshot_hadoop['CreationDate']).dt.strftime('%Y-%m-%d')
df_snapshot_hadoop['CreationDateDay'] = pd.to_datetime(df_snapshot_hadoop['CreationDateDay'])

#df_snapshot_hadoop = format_string_to_date(df_snapshot_hadoop, 'ResolutionDate')
df_snapshot_hadoop['ResolutionDate'] = pd.to_datetime(df_snapshot_hadoop['ResolutionDate'], dayfirst=True).dt.strftime('%Y-%m-%d %H:%M:%S')
df_snapshot_hadoop['ResolutionDate'] = pd.to_datetime(df_snapshot_hadoop['ResolutionDate'])

df_snapshot_hadoop['ResolutionDateDay'] = pd.to_datetime(df_snapshot_hadoop['ResolutionDate']).dt.strftime('%Y-%m-%d')
df_snapshot_hadoop['ResolutionDateDay'] = pd.to_datetime(df_snapshot_hadoop['ResolutionDateDay'])

df_snapshot_hadoop['DiffDates'] = df_snapshot_hadoop['ResolutionDate'] - df_snapshot_hadoop['CreationDate']

df_snapshot_hadoop['DiffDatesDay'] = df_snapshot_hadoop['ResolutionDateDay'] - df_snapshot_hadoop['CreationDateDay']

df_snapshot_hadoop['DiffDatesDay']

#FirstCommentDate
#df_snapshot_hadoop = format_string_to_date(df_snapshot_hadoop, 'FirstCommentDate')
df_snapshot_hadoop['FirstCommentDate'] = pd.to_datetime(df_snapshot_hadoop['FirstCommentDate'], dayfirst=True).dt.strftime('%Y-%m-%d %H:%M:%S')
df_snapshot_hadoop['FirstCommentDate'] = pd.to_datetime(df_snapshot_hadoop['FirstCommentDate'])

#LastCommentDate
#df_snapshot_hadoop = format_string_to_date(df_snapshot_hadoop, 'LastCommentDate')
df_snapshot_hadoop['LastCommentDate'] = pd.to_datetime(df_snapshot_hadoop['LastCommentDate'], dayfirst=True).dt.strftime('%Y-%m-%d %H:%M:%S')
df_snapshot_hadoop['LastCommentDate'] = pd.to_datetime(df_snapshot_hadoop['LastCommentDate'])

df_snapshot_hadoop['DiffCommentDates'] = df_snapshot_hadoop['LastCommentDate'] - df_snapshot_hadoop['FirstCommentDate']

df_snapshot_hadoop.sort_values(by=['DiffDates'], ascending=True).head(3)

"""**10. Statistical data about df_snapshot_hadoop cleaning and transformed**"""

df_snapshot_hadoop.describe()

"""**Graphics about df_snapshot_hadoop**"""

df_test_creationdate = create_dataset_sum_groupby_column(df_snapshot_hadoop, 'CreationDateDay')
df_test_creationdate.sort_values(by=['CreationDateDay'], inplace=True)

configure_plotly_browser_state()

trace = go.Scatter(x = df_test_creationdate['CreationDateDay'],
                   y = df_test_creationdate['Qtd'],
                   mode = 'markers')

data = [trace]

layout = go.Layout(title='Relação de Número de Bugs por data de criação',
                   yaxis={'title':'Quantidade de Bugs'},
                   xaxis={'title': 'Ano da Criação'})
fig = go.Figure(data=data, layout=layout)
py.iplot(fig)

"""**Relação de Números de Comentários por data**"""

configure_plotly_browser_state()

trace = go.Scatter(x = df_snapshot_hadoop['FirstCommentDate'],
                   y = df_snapshot_hadoop['NoComments'],
                   mode = 'markers')

data = [trace]

layout = go.Layout(title='Relação de Número de Comentários por data de criação do comentário',
                   yaxis={'title':'Quantidade de comentários'},
                   xaxis={'title': 'Ano do comentário'})
fig = go.Figure(data=data, layout=layout)
py.iplot(fig)

configure_plotly_browser_state()
# Using plotly.express
import plotly.express as px

fig = px.line(cleaned_aux_df_snapshot_hadoop, x='FirstCommentDate', y='NoComments')
py.iplot(fig)

#df_snapshot_hadoop
configure_plotly_browser_state()
# Using plotly.express
import plotly.express as px

fig = px.line(df_snapshot_hadoop, x='FirstCommentDate', y='NoComments')
py.iplot(fig)

df_snapshot_hadoop['Priority'].unique()

configure_plotly_browser_state()

# Gerando gráficos para Prioridade do tipo Major
trace1 = go.Box(y = df_snapshot_hadoop.loc[df_snapshot_hadoop['Priority'] == 'Major', 'NoComments'],
                name = 'Prioridade do tipo Major',
                marker = {'color': '#f39c12'})
# Gráfico de caixa para Prioridade do tipo Blocker
trace2 = go.Box(y = df_snapshot_hadoop.loc[df_snapshot_hadoop['Priority'] == 'Blocker', 'NoComments'],
                name = 'Prioridade do tipo Blocker',
                marker = {'color': '#e67e22'})

# Gráfico de caixa para Prioridade do tipo Critical
trace3 = go.Box(y = df_snapshot_hadoop.loc[df_snapshot_hadoop['Priority'] == 'Critical', 'NoComments'],
                name = 'Prioridade do tipo Critical',
                marker = {'color': '#d35400'})

# Gráfico para Prioridade do Tipo Trivial
trace4 = go.Box(y = df_snapshot_hadoop.loc[df_snapshot_hadoop['Priority'] == 'Trivial', 'NoComments'],
                name = 'Prioridade do tipo Trivial',
                marker = {'color': '#e74c3c'})

data = [trace1, trace2, trace3, trace4]
layout = go.Layout(title = 'Dispersão de comentários com diferentes prioridades',
                   titlefont = {'family': 'Arial',
                                'size': 22,
                                'color': '#7f7f7f'},
                   xaxis = {'title': 'Tipo de Prioridade'},
                   yaxis = {'title': 'Quantidade de Comenários'},
                   paper_bgcolor = 'rgb(243, 243, 243)',
                   plot_bgcolor = 'rgb(243, 243, 243)')

fig = go.Figure(data=data, layout=layout)
py.iplot(fig)

"""**Número de Comentários**"""

df_snapshot_hadoop.NoComments.plot(kind='hist')

"""**Número de Comentários agrupados por Prioridade**"""

sns.boxplot(x='Priority', y='NoComments', data=df_snapshot_hadoop)

"""**Relação entre número de comentários e Quantidade de Arquivos Modificados**"""

sns.lmplot(x='NoComments', y='SrcModFiles', data=df_snapshot_hadoop)

# Basic correlogram
#sns.pairplot(cleaned_df_snapshot_hadoop)
#plt.show()

"""### 2.2 Changelog"""

df_changelog_hadoop.head(3)

df_changelog_hadoop.info()

## Quantidade de autores unicos que fizeram alteracoes
len(df_changelog_hadoop.Author.unique())

## Agrupa changelogs por Author
my_summarized_by_author = create_dataset_sum_groupby_column(df_changelog_hadoop, 'Author')
my_summarized_by_author.head(10)

## Quantidade de issues unicos
len(df_changelog_hadoop.Key.unique())

df_changelog_hadoop.Key.unique()

## Agrupa por Issues
##group_by_issue = df_changelog_hadoop.groupby('Key')
my_df_summarized_by_issue = create_dataset_sum_groupby_column(df_changelog_hadoop, 'Key')

my_df_summarized_by_issue.head(10)

## Agrupa por Field
my_df_summarized_by_field = create_dataset_sum_groupby_column(df_changelog_hadoop, 'Field')
my_df_summarized_by_field.head(10)

len(df_changelog_hadoop.From.unique())

df_changelog_hadoop.From.unique()

len(df_changelog_hadoop.To.unique())

df_changelog_hadoop.To.unique()

"""### 2.3 Commentlog"""

df_commentlog_hadoop.head(3)

df_commentlog_hadoop.info()

df_commentlog_summarized_by_key = create_dataset_sum_groupby_column(df_commentlog_hadoop, 'Key')
df_commentlog_summarized_by_author = create_dataset_sum_groupby_column(df_commentlog_hadoop, 'Author')

df_commentlog_summarized_by_author.head(10)

df_commentlog_summarized_by_key.head(10)

df_commentlog_hadoop.Key[2]

df_commentlog_hadoop.Content[2]

"""### 2.4 Commitlog"""

df_commitlog_hadoop.head(3)

df_commitlog_hadoop.describe()

df_commitlog_hadoop.query('Key in "HADOOP-6096"').FilePath

for item in df_commitlog_hadoop.FilePath.unique():
  print(item)

df_commitlog_hadoop.info()

df_commitlog_hadoop.describe()

df_commitlog_summarized_by_key = create_dataset_sum_groupby_column(df_commitlog_hadoop, 'Key')
df_commitlog_summarized_by_author = create_dataset_sum_groupby_column(df_commitlog_hadoop, 'Author')
df_commitlog_summarized_by_changetype = create_dataset_sum_groupby_column(df_commitlog_hadoop, 'ChangeType')

df_commitlog_summarized_by_author.head(10)

df_commitlog_summarized_by_key.head(10)

df_commitlog_summarized_by_changetype

"""## 3. Mapreduce Bug-Fix

### 3.1 Snapshot
"""

df_snapshot_mapreduce.head(3)

df_snapshot_mapreduce.drop(['Unnamed: 0'], axis=1, inplace=True)

df_snapshot_mapreduce.head(3)

df_snapshot_mapreduce.info()

df_snapshot_mapreduce.describe()

df_snapshot_mapreduce.columns

#1. Fazer o replacing das colunas tipo string null para unknown

df_snapshot_mapreduce.isnull().sum()
#**1. df Columns with data null**

columsn_with_null_elements_mapreduce = df_snapshot_mapreduce.isna().any()[lambda x: x]
columsn_with_null_elements_mapreduce

null_cols_df_mapreduce = [item for item in df_snapshot_mapreduce.columns if df_snapshot_mapreduce[item].isnull().any()]
null_cols_df_mapreduce

df_snapshot_mapreduce[null_cols_df_mapreduce].head(3)

#**2. Replacing important columns null data values with unknown**

df_snapshot_mapreduce.Assignee.fillna('unknown', inplace=True)
df_snapshot_mapreduce.Components.fillna('unknown', inplace=True)
df_snapshot_mapreduce.SummaryTopWords.fillna('unknown', inplace=True)
df_snapshot_mapreduce.DescriptionTopWords.fillna('unknown', inplace=True)
df_snapshot_mapreduce.CommentsTopWords.fillna('unknown', inplace=True)
df_snapshot_mapreduce.AffectsVersions.fillna('unknown', inplace=True)
df_snapshot_mapreduce.FixVersions.fillna('unknown', inplace=True)
df_snapshot_mapreduce.CommitsMessagesTopWords.fillna('unknown', inplace=True)

#3. Remover as colunas com muitos valores nulls

df_snapshot_mapreduce.isnull().sum()

column_with_lot_of_nulls_mapreduce = ['FirstAttachmentDate', 'LastAttachmentDate', 'FirstAttachedPatchDate', 'LastAttachedPatchDate', 'InwardIssueLinks', 'OutwardIssueLinks']

#**3. Removing column_with_lot_of_nulls**

df_snapshot_mapreduce.drop(column_with_lot_of_nulls_mapreduce, axis=1, inplace=True)

df_snapshot_mapreduce.isnull().sum()

#**4. Removing rows with null values and reseting main index**

df_snapshot_mapreduce.dropna(how='any', inplace=True)
df_snapshot_mapreduce.reset_index(drop=True, inplace=True)

#5. Transformação de Dados

#**5. Cleaned data Aux to record df partial cleaned**

cleaned_aux_df_snapshot_mapreduce = df_snapshot_mapreduce

#**6. Date transformation and creation of new columns about date**

df_snapshot_mapreduce['CreationDate'] = pd.to_datetime(df_snapshot_mapreduce['CreationDate'], dayfirst=True).dt.strftime('%Y-%m-%d %H:%M:%S')
df_snapshot_mapreduce['CreationDate'] = pd.to_datetime(df_snapshot_mapreduce['CreationDate'])

df_snapshot_mapreduce['CreationDateDay'] = pd.to_datetime(df_snapshot_mapreduce['CreationDate']).dt.strftime('%Y-%m-%d')
df_snapshot_mapreduce['CreationDateDay'] = pd.to_datetime(df_snapshot_mapreduce['CreationDateDay'])

df_snapshot_mapreduce['ResolutionDate'] = pd.to_datetime(df_snapshot_mapreduce['ResolutionDate'], dayfirst=True).dt.strftime('%Y-%m-%d %H:%M:%S')
df_snapshot_mapreduce['ResolutionDate'] = pd.to_datetime(df_snapshot_mapreduce['ResolutionDate'])

df_snapshot_mapreduce['ResolutionDateDay'] = pd.to_datetime(df_snapshot_mapreduce['ResolutionDate']).dt.strftime('%Y-%m-%d')
df_snapshot_mapreduce['ResolutionDateDay'] = pd.to_datetime(df_snapshot_mapreduce['ResolutionDateDay'])

df_snapshot_mapreduce['DiffDates'] = df_snapshot_mapreduce['ResolutionDate'] - df_snapshot_mapreduce['CreationDate']

df_snapshot_mapreduce['DiffDatesDay'] = df_snapshot_mapreduce['ResolutionDateDay'] - df_snapshot_mapreduce['CreationDateDay']

df_snapshot_mapreduce['DiffDatesDay']

#FirstCommentDate
df_snapshot_mapreduce['FirstCommentDate'] = pd.to_datetime(df_snapshot_mapreduce['FirstCommentDate'], dayfirst=True).dt.strftime('%Y-%m-%d %H:%M:%S')
df_snapshot_mapreduce['FirstCommentDate'] = pd.to_datetime(df_snapshot_mapreduce['FirstCommentDate'])

#LastCommentDate
df_snapshot_mapreduce['LastCommentDate'] = pd.to_datetime(df_snapshot_mapreduce['LastCommentDate'], dayfirst=True).dt.strftime('%Y-%m-%d %H:%M:%S')
df_snapshot_mapreduce['LastCommentDate'] = pd.to_datetime(df_snapshot_mapreduce['LastCommentDate'])

df_snapshot_mapreduce['DiffCommentDates'] = df_snapshot_mapreduce['LastCommentDate'] - df_snapshot_mapreduce['FirstCommentDate']

df_snapshot_mapreduce.sort_values(by=['DiffDates'], ascending=True).head(3)

#**7. Statistical data about df cleaning and transformed**

df_snapshot_mapreduce.describe()

"""## 4. HDFS Bug-Fix

### 4.1 Snapshot
"""

df_snapshot_hdfs.head(3)

df_snapshot_hdfs.drop(['Unnamed: 0'], axis=1, inplace=True)

df_snapshot_hdfs.head(3)

df_snapshot_hdfs.info()

df_snapshot_hdfs.describe()

#1. Fazer o replacing das colunas tipo string null para unknown

df_snapshot_hdfs.isnull().sum()
#**1. df Columns with data null**

columsn_with_null_elements_hdfs = df_snapshot_hdfs.isna().any()[lambda x: x]
columsn_with_null_elements_hdfs

null_cols_df_hdfs = [item for item in df_snapshot_hdfs.columns if df_snapshot_hdfs[item].isnull().any()]
null_cols_df_hdfs

df_snapshot_hdfs[null_cols_df_hdfs].head(3)

#**2. Replacing important columns null data values with unknown**

df_snapshot_hdfs.Assignee.fillna('unknown', inplace=True)
df_snapshot_hdfs.Components.fillna('unknown', inplace=True)
df_snapshot_hdfs.SummaryTopWords.fillna('unknown', inplace=True)
df_snapshot_hdfs.DescriptionTopWords.fillna('unknown', inplace=True)
df_snapshot_hdfs.CommentsTopWords.fillna('unknown', inplace=True)
df_snapshot_hdfs.AffectsVersions.fillna('unknown', inplace=True)
df_snapshot_hdfs.FixVersions.fillna('unknown', inplace=True)
df_snapshot_hdfs.CommitsMessagesTopWords.fillna('unknown', inplace=True)

#3. Remover as colunas com muitos valores nulls

df_snapshot_hdfs.isnull().sum()

column_with_lot_of_nulls_hdfs = ['FirstAttachmentDate', 'LastAttachmentDate', 'FirstAttachedPatchDate', 'LastAttachedPatchDate', 'InwardIssueLinks', 'OutwardIssueLinks']

#**3. Removing column_with_lot_of_nulls**

df_snapshot_hdfs.drop(column_with_lot_of_nulls_hdfs, axis=1, inplace=True)

df_snapshot_hdfs.isnull().sum()

#**4. Removing rows with null values and reseting main index**

df_snapshot_hdfs.dropna(how='any', inplace=True)
df_snapshot_hdfs.reset_index(drop=True, inplace=True)

#5. Transformação de Dados

#**5. Cleaned data Aux to record df partial cleaned**

cleaned_aux_df_snapshot_hdfs = df_snapshot_hdfs

#**6. Date transformation and creation of new columns about date**

df_snapshot_hdfs['CreationDate'] = pd.to_datetime(df_snapshot_hdfs['CreationDate'], dayfirst=True).dt.strftime('%Y-%m-%d %H:%M:%S')
df_snapshot_hdfs['CreationDate'] = pd.to_datetime(df_snapshot_hdfs['CreationDate'])

df_snapshot_hdfs['CreationDateDay'] = pd.to_datetime(df_snapshot_hdfs['CreationDate']).dt.strftime('%Y-%m-%d')
df_snapshot_hdfs['CreationDateDay'] = pd.to_datetime(df_snapshot_hdfs['CreationDateDay'])

df_snapshot_hdfs['ResolutionDate'] = pd.to_datetime(df_snapshot_hdfs['ResolutionDate'], dayfirst=True).dt.strftime('%Y-%m-%d %H:%M:%S')
df_snapshot_hdfs['ResolutionDate'] = pd.to_datetime(df_snapshot_hdfs['ResolutionDate'])

df_snapshot_hdfs['ResolutionDateDay'] = pd.to_datetime(df_snapshot_hdfs['ResolutionDate']).dt.strftime('%Y-%m-%d')
df_snapshot_hdfs['ResolutionDateDay'] = pd.to_datetime(df_snapshot_hdfs['ResolutionDateDay'])

df_snapshot_hdfs['DiffDates'] = df_snapshot_hdfs['ResolutionDate'] - df_snapshot_hdfs['CreationDate']

df_snapshot_hdfs['DiffDatesDay'] = df_snapshot_hdfs['ResolutionDateDay'] - df_snapshot_hdfs['CreationDateDay']

df_snapshot_hdfs['DiffDatesDay']

#FirstCommentDate
df_snapshot_hdfs['FirstCommentDate'] = pd.to_datetime(df_snapshot_hdfs['FirstCommentDate'], dayfirst=True).dt.strftime('%Y-%m-%d %H:%M:%S')
df_snapshot_hdfs['FirstCommentDate'] = pd.to_datetime(df_snapshot_hdfs['FirstCommentDate'])

#LastCommentDate
df_snapshot_hdfs['LastCommentDate'] = pd.to_datetime(df_snapshot_hdfs['LastCommentDate'], dayfirst=True).dt.strftime('%Y-%m-%d %H:%M:%S')
df_snapshot_hdfs['LastCommentDate'] = pd.to_datetime(df_snapshot_hdfs['LastCommentDate'])

df_snapshot_hdfs['DiffCommentDates'] = df_snapshot_hdfs['LastCommentDate'] - df_snapshot_hdfs['FirstCommentDate']

df_snapshot_hdfs.sort_values(by=['DiffDates'], ascending=True).head(3)

#**7. Statistical data about df cleaning and transformed**

df_snapshot_hdfs.describe()

"""## 5. YARN Bug-Fix

### 5.1 Snapshot
"""

df_snapshot_yarn.head(3)

df_snapshot_yarn.drop(['Unnamed: 0'], axis=1, inplace=True)

df_snapshot_yarn.head(3)

df_snapshot_yarn.info()

df_snapshot_yarn.describe()

#1. Fazer o replacing das colunas tipo string null para unknown

df_snapshot_yarn.isnull().sum()
#**1. df Columns with data null**

columsn_with_null_elements_yarn = df_snapshot_yarn.isna().any()[lambda x: x]
columsn_with_null_elements_yarn

null_cols_df_yarn = [item for item in df_snapshot_yarn.columns if df_snapshot_yarn[item].isnull().any()]
null_cols_df_yarn

df_snapshot_yarn[null_cols_df_yarn].head(3)

#**2. Replacing important columns null data values with unknown**

df_snapshot_yarn.Assignee.fillna('unknown', inplace=True)
df_snapshot_yarn.Components.fillna('unknown', inplace=True)
df_snapshot_yarn.SummaryTopWords.fillna('unknown', inplace=True)
df_snapshot_yarn.DescriptionTopWords.fillna('unknown', inplace=True)
df_snapshot_yarn.CommentsTopWords.fillna('unknown', inplace=True)
df_snapshot_yarn.AffectsVersions.fillna('unknown', inplace=True)
df_snapshot_yarn.FixVersions.fillna('unknown', inplace=True)
df_snapshot_yarn.CommitsMessagesTopWords.fillna('unknown', inplace=True)

#3. Remover as colunas com muitos valores nulls

df_snapshot_yarn.isnull().sum()

column_with_lot_of_nulls_yarn = ['FirstAttachmentDate', 'LastAttachmentDate', 'FirstAttachedPatchDate', 'LastAttachedPatchDate', 'InwardIssueLinks', 'OutwardIssueLinks']

#**3. Removing column_with_lot_of_nulls**

df_snapshot_yarn.drop(column_with_lot_of_nulls_yarn, axis=1, inplace=True)

df_snapshot_yarn.isnull().sum()

#**4. Removing rows with null values and reseting main index**

df_snapshot_yarn.dropna(how='any', inplace=True)
df_snapshot_yarn.reset_index(drop=True, inplace=True)

#5. Transformação de Dados

#**5. Cleaned data Aux to record df partial cleaned**

cleaned_aux_df_snapshot_yarn = df_snapshot_yarn

#**6. Date transformation and creation of new columns about date**

df_snapshot_yarn['CreationDate'] = pd.to_datetime(df_snapshot_yarn['CreationDate'], dayfirst=True).dt.strftime('%Y-%m-%d %H:%M:%S')
df_snapshot_yarn['CreationDate'] = pd.to_datetime(df_snapshot_yarn['CreationDate'])

df_snapshot_yarn['CreationDateDay'] = pd.to_datetime(df_snapshot_yarn['CreationDate']).dt.strftime('%Y-%m-%d')
df_snapshot_yarn['CreationDateDay'] = pd.to_datetime(df_snapshot_yarn['CreationDateDay'])

df_snapshot_yarn['ResolutionDate'] = pd.to_datetime(df_snapshot_yarn['ResolutionDate'], dayfirst=True).dt.strftime('%Y-%m-%d %H:%M:%S')
df_snapshot_yarn['ResolutionDate'] = pd.to_datetime(df_snapshot_yarn['ResolutionDate'])

df_snapshot_yarn['ResolutionDateDay'] = pd.to_datetime(df_snapshot_yarn['ResolutionDate']).dt.strftime('%Y-%m-%d')
df_snapshot_yarn['ResolutionDateDay'] = pd.to_datetime(df_snapshot_yarn['ResolutionDateDay'])

df_snapshot_yarn['DiffDates'] = df_snapshot_yarn['ResolutionDate'] - df_snapshot_yarn['CreationDate']

df_snapshot_yarn['DiffDatesDay'] = df_snapshot_yarn['ResolutionDateDay'] - df_snapshot_yarn['CreationDateDay']

df_snapshot_yarn['DiffDatesDay']

#FirstCommentDate
df_snapshot_yarn['FirstCommentDate'] = pd.to_datetime(df_snapshot_yarn['FirstCommentDate'], dayfirst=True).dt.strftime('%Y-%m-%d %H:%M:%S')
df_snapshot_yarn['FirstCommentDate'] = pd.to_datetime(df_snapshot_yarn['FirstCommentDate'])

#LastCommentDate
df_snapshot_yarn['LastCommentDate'] = pd.to_datetime(df_snapshot_yarn['LastCommentDate'], dayfirst=True).dt.strftime('%Y-%m-%d %H:%M:%S')
df_snapshot_yarn['LastCommentDate'] = pd.to_datetime(df_snapshot_yarn['LastCommentDate'])

df_snapshot_yarn['DiffCommentDates'] = df_snapshot_yarn['LastCommentDate'] - df_snapshot_yarn['FirstCommentDate']

df_snapshot_yarn.sort_values(by=['DiffDates'], ascending=True).head(3)

#**7. Statistical data about df cleaning and transformed**

df_snapshot_yarn.describe()

"""# Concat all snapshot dataframes"""

frames = [df_snapshot_hadoop, df_snapshot_mapreduce, df_snapshot_hdfs, df_snapshot_yarn]

result = pd.concat(frames, ignore_index=True)

result

#You have to mount your drive and authenticate...
#from google.colab import drive
#drive.mount('drive')

#result.to_csv('concat_snapshot_all_hadoop.csv', encoding='utf-8', index=False)
#!cp concat_snapshot_all_hadoop.csv drive/My\ Drive/

#result.to_csv('concat_snapshot_all_hadoop_pv.csv', sep=';', encoding='utf-8', index=False)
#!cp concat_snapshot_all_hadoop_pv.csv drive/My\ Drive/